{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Tutorial\n",
    "\n",
    "This notebook serves as a tutorial for the `ugali` pipeline analysis. Its purpose is to document the steps taken to perform an end-to-end likelihood analysis on a raw data set. The ugali pipeline is comprised of the following steps:\n",
    "\n",
    "* `run_01.0_download_data.py` - This step is used for downloading data from the database\n",
    "* `run_02.0_preprocess.py` - This step preprocesses the input data into the structure ugali expects\n",
    "* `run_03.0_likelihood.py` - This step runs the likelihood grid search over the data set\n",
    "* `run_04.0_peak_finder.py` - Takes the output likelihood grid and finds peaks in 3D\n",
    "* `run_05.0_followup.py` - This step runs mcmc parameter fitting\n",
    "* `run_06.0_simulate.py` - Runs and analyzes simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Setup\n",
    "\n",
    "Our first step is to do some generic notebook imports, create a working directory, and install some test data from github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import healpy as hp\n",
    "import fitsio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the working directory and chdir into it\n",
    "!mkdir -p work\n",
    "os.chdir('./work')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the test data\n",
    "!wget https://github.com/DarkEnergySurvey/ugali/releases/download/v1.7.0/ugali-test-data.tar.gz\n",
    "!tar -xzf ugali-test-data.tar.gz && rm -f ugali-test-data.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For easier access, we'll create symlinks to some of the `ugali` components that we'll be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Links to package components\n",
    "!rm -f ugali && ln -s ../ugali\n",
    "!rm -f pipeline && ln -s ugali/pipeline\n",
    "# Link to configuration and source model files\n",
    "!rm -f config.yaml && ln -s ../tests/config.yaml\n",
    "!rm -f srcmdl.yaml && ln -s ../tests/srcmdl.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what we've got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The components of our working directory are now:\n",
    "* `healpix` - this directory contains the catalog data divided into files by healpix pixel (and labeled as such)\n",
    "* `mask` - mask of the magnitude limit in \"sparse\" healpix representation corresponding to each catalog pixel\n",
    "* `pipeline` - pipeline scripts\n",
    "* `config.yaml` - pipeline configuration file\n",
    "* `srcmdl.yaml` - source model file\n",
    "\n",
    "We can examine some of these constituents to get a better feel for the files we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate the catalog\n",
    "data = fitsio.read('healpix/catalog_hpx0687.fits')\n",
    "print(data.dtype.names)\n",
    "print(data[:3])\n",
    "\n",
    "# Displaying the catalog as a healpix map\n",
    "nside=4096\n",
    "c = np.zeros(hp.nside2npix(nside))\n",
    "pix,cts = np.unique(data['PIX4096'],return_counts=True)\n",
    "c[pix] = cts\n",
    "\n",
    "hp.cartview(c,lonra=[53.5,55],latra=[-55,-53.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate the magnitude limit mask\n",
    "mask = fitsio.read('mask/maglim_g_hpx0687.fits')\n",
    "print(mask.dtype.names)\n",
    "print(mask[:3])\n",
    "\n",
    "# Displaying the catalog as a healpix map\n",
    "m = np.zeros(hp.nside2npix(nside))\n",
    "m[mask['PIXEL']] = mask['MAGLIM']\n",
    "\n",
    "hp.cartview(m,lonra=[53.5,55],latra=[-55,-53.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration \n",
    "\n",
    "The configuration file is the key to the ugali pipeline analysis. We are going to use [`config.yaml`](https://github.com/DarkEnergySurvey/ugali/blob/master/tests/config.yaml). This file contains the path to the catalog and mask files that we will use, as well as other configuration parameters for the various pipeline steps. In subsequent steps we will refer to specific configuration parameters in this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 25 config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Downloading Data\n",
    "\n",
    "While there is a pipeline step for downloading some data sets (i.e., from SDSS and DES), this is mostly vistigial code that has been left as an example for the user. In almost all cases that user will provide their own data set from some arbitrary source of data.\n",
    "\n",
    "The requirements of the input data are kept fairly minimal, and can be found in the `config['catalog']` section:\n",
    "```\n",
    "catalog:\n",
    "  dirname: ./healpix                    # directory of catalog files                 \n",
    "  basename: \"catalog_hpx%04i.fits\"      # catalog file basename format               \n",
    "  objid_field       : COADD_OBJECT_ID   # unique object identifier                   \n",
    "  lon_field         : RA                # name of longitude field                    \n",
    "  lat_field         : DEC               # name of latitude field                     \n",
    "  # Color always defined as mag_1 - mag_2                                            \n",
    "  mag_1_band        : &band1 g          # name of mag_1 filter band                  \n",
    "  mag_1_field       : WAVG_MAG_PSF_G    # name of mag_1 field                        \n",
    "  mag_err_1_field   : WAVG_MAGERR_PSF_G # name of magerr_1 field                     \n",
    "  mag_2_band        : &band2 r          # name of mag_2 filter band                  \n",
    "  mag_2_field       : WAVG_MAG_PSF_R    # name of mag_2 field                        \n",
    "  mag_err_2_field   : WAVG_MAGERR_PSF_R # name of magerr_2 field                     \n",
    "  # True = band 1 is detection band; False = band 2 is detection band                \n",
    "  band_1_detection  : &detection True   # detection occurs in band_1?                \n",
    "  mc_source_id_field: MC_SOURCE_ID      # field for simulated objects                \n",
    "  # Additional selection parameters to get a sample of stars                         \n",
    "  selection         : \"(np.abs(self.data['WAVG_SPREAD_MODEL_I']) < 0.003)\"\n",
    "```\n",
    "\n",
    "* `objid_field` - This is some unique identifier for objects in the catalog\n",
    "* `lon_field, lat_field` - The longitude and latitude of each object in the catalogs. This is usually `RA` and `DEC` with `coordsys = CEL`; however, Galactic coordinates are also supported (though less well tested).\n",
    "* `mag_[1,2]_band, mag_[1,2]_field, mag_err_[1,2]_field` - These are columns corresponding to the magnitude and magnitude error of each object in the catalog. Magnitudes are assumed to be extiniction corrected.\n",
    "* `selection` - This column can be used to specify any additional selection (e.g., star-galaxy classification, quality cuts) that need to be applied to the catalog before analyzing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Preprocessing\n",
    "\n",
    "In order to run `ugali` the data needs to be `preprocessed` into a prescribed directory structure. This step involves assembling the data products into a file structure that ugali assumes. This step in the pipeline makes use of the `run_02.0_preprocess.py` script. You can run one step at a time or multiple steps at once using the `--run <XXX>` option. For example:\n",
    "```\n",
    "python pipeline/run_02.0_preprocess.py config.yaml -r pixelize\n",
    "```\n",
    "will run just `pixelize`, while\n",
    "```\n",
    "python pipeline/run_02.0_preprocess.py config.yaml -r pixelize -r split\n",
    "```\n",
    "will run both `pixelize` and `split` in order.\n",
    "\n",
    "This is a general feature of the `ugali` pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.1: Pixelizing the data\n",
    "\n",
    "Input data can be organized in any arbitrary structure; however, `ugali` expects the data to be spatially sorted into healpix pixels and stored in files with appropriate names. The input catalog data files are specified in `config.yaml` in `config['data']['dirname']`\n",
    "```\n",
    "data:\n",
    "  dirname: /home/s1/kadrlica/projects/bliss/dsphs/v2/raw\n",
    "```\n",
    "The code will `glob` for all files in `dirname` ending in `*.fits`.  \n",
    "\n",
    "The output resolution of the pixelized catalog files is defined in `config.yaml`\n",
    "```\n",
    "coords:\n",
    "  nside_catalog   : 32     # Size of patches for catalog binning\n",
    "```\n",
    "The output pixelized files will be placed in `config['catalog']['dirname']` conforming to `config['catalog']['basename']`.\n",
    "```\n",
    "catalog:\n",
    "  dirname: /home/s1/kadrlica/projects/bliss/dsphs/v2/healpix\n",
    "  basename: \"cat_hpx_%05i.fits\"\n",
    "```\n",
    "\n",
    "In this test case the input data is already pixelized, so this is a trivial operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python pipeline/run_02.0_preprocess.py config.yaml -r pixelize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.2: Creating the magnitude limit mask\n",
    "\n",
    "Ugali relies on some knowledge of the footprint of the data being analyzed. This is used to calculated the model predicted stellar counts and to exclude regions that are uncovered by a specific data set.\n",
    "\n",
    "The mask takes as input the footprint of the data set, specified in:\n",
    "```\n",
    "data:\n",
    "  footprint: ./maps/footprint_n4096_equ.fits.gz\n",
    "```\n",
    "\n",
    "The magnitude limit mask can be derived from the footprint in 3 different ways:\n",
    "1. A uniform maglim for every pixel in the footprint (this default maglim is set per survey in `ugali/utils/constants.py`). This is run with the `-r simple` option to `run_02.0_preprocess.py`.\n",
    "2. Derived from the data using the the magnitude error. This is set with the `-r maglim` option\n",
    "3. From a pre-derived magnitude limit file. This is just splitting an all-sky healpix map into a set of partial healpix maps.\n",
    "\n",
    "The output of this step will be placed in:\n",
    "```\n",
    "mask:\n",
    "  dirname    : ./mask\n",
    "  basename_1 : \"maglim_g_hpx%04i.fits\"\n",
    "  basename_2 : \"maglim_r_hpx%04i.fits\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python pipeline/run_02.0_preprocess.py config.yaml -r simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Likelihood Grid Scan\n",
    "\n",
    "Our next step will be to perform a likelihood grid scan over our test data set. Before executing the script, let's figure out what the arguments are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python pipeline/run_03.0_likelihood.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command that we will run is\n",
    "```\n",
    "python pipeline/run_03.0_likelihood.py config.yaml -r scan -q local\n",
    "```\n",
    "Breaking this down:\n",
    "* `python pipeline/run_03.0_likelihood.py` - the script itself\n",
    "* `config.yaml` - the first positional argument of all the pipeline scripts is the configuration file\n",
    "* `-r scan` - the component of the pipeline we want to run; in this case the grid scan\n",
    "* `-q local` - we are going to execute this component locally\n",
    "\n",
    "Now to run the script..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python pipeline/run_03.0_likelihood.py config.yaml -r scan -q local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will take a while for the script to run. The result will be a set of output files in the `scan` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh scan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to merge the output likelihood files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python pipeline/run_03.0_likelihood.py config.yaml -r merge -q local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../work')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Find Likelihood Candidates\n",
    "\n",
    "After the likelihood scan is run we are left with a merged healpix file containing the likelihood evaluated at each healpix coordinate and distance modulus. We can search this likelihood cube for over densities in the likelihood corresponding to satellite candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python pipeline/run_04.0_peak_finder.py config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Fit Candidate Parameters\n",
    "\n",
    "After identifying satellite candidates, we would like to fit their observed parameters (location, distance, size, luminosity, etc.). This is implemented by calculating the posterior probability with MCMC sampling. The first step is to create a \"source model\" file for parameterizing the parameters of the satelliite model. An example source model is provided in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python pipeline/run_05.0_followup.py config.yaml -t targets.txt -q local -r mcmc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
